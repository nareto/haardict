from haardict import *
import numpy as np
import sys
import pandas as pd
import ipdb
import gc
from cycler import cycler

np.random.seed(123)

MARKERSIZE = 6
cdash = [1,0.5]
hdash = [3,1]
cwidth = 1
hwidth = 1.6

COLORMAP = {
    'plain': 				{'lwidth': 1, 		'marker': None, 'dash': (None,None)},
    'ksvd': 				{'lwidth': 1, 		'marker': '+', 'dash': (None,None)},
    'centroids-dict-twomeans': 		{'lwidth': cwidth, 	'marker': '$m$', 'dash': cdash},
    'haar-dict-twomeans': 	       	{'lwidth': hwidth, 	'marker': '$m$', 'dash': hdash},
    'centroids-dict-twomaxoids': 	{'lwidth': cwidth, 	'marker': '$M$', 'dash': cdash},
    'haar-dict-twomaxoids': 		{'lwidth': hwidth, 	'marker': '$M$', 'dash': hdash},
    #'centroids-dict-twomeans': 		{'lwidth': cwidth, 	'marker': '*', 'dash': cdash},
    #'haar-dict-twomeans': 	       	{'lwidth': hwidth, 	'marker': '*', 'dash': hdash},
    #'centroids-dict-twomaxoids': 	{'lwidth': cwidth, 	'marker': '^', 'dash': cdash},
    #'haar-dict-twomaxoids': 		{'lwidth': hwidth, 	'marker': '^', 'dash': hdash},
    #'haar-dict-spectral-emd': 		{'lwidth': hwidth, 	'marker': '$\diamond$', 'dash': hdash},
    #'centroids-dict-spectral-emd': 		{'lwidth': cwidth, 	'marker': '$\diamond$', 'dash': cdash},
    #'haar-dict-spectral-emd': 		{'lwidth': hwidth, 	'marker': 'd', 'dash': hdash},
    #'centroids-dict-spectral-emd': 		{'lwidth': cwidth, 	'marker': 'd', 'dash': cdash},
    'haar-dict-spectral-emd': 		{'lwidth': hwidth, 	'marker': '$E$', 'dash': hdash},
    'centroids-dict-spectral-emd':	{'lwidth': cwidth, 	'marker': '$E$', 'dash': cdash},
    'haar-dict-spectral-frobenius': 	{'lwidth': hwidth, 	'marker': '$F$', 'dash': hdash},
    'centroids-dict-spectral-frobenius':{'lwidth': cwidth, 	'marker': '$F$', 'dash': cdash},
    'haar-dict-spectral-haarpsi': 	{'lwidth': hwidth, 	'marker': '$H$', 'dash': hdash},
    'centroids-dict-spectral-haarpsi': 	{'lwidth': cwidth, 	'marker': '$H$', 'dash': cdash} 
    #'haar-dict-spectral-frobenius': 	{'lwidth': hwidth, 	'marker': '|', 'dash': hdash},
    #'centroids-dict-spectral-frobenius': 	{'lwidth': cwidth, 	'marker': '|', 'dash': cdash},
    #'haar-dict-spectral-haarpsi': 		{'lwidth': hwidth, 	'marker': '1', 'dash': hdash},
    #'centroids-dict-spectral-haarpsi': 	{'lwidth': cwidth, 	'marker': '1', 'dash': cdash} 
    }

def _filter_columns(df,index='n.patches',psize=None,reconstruction_sparsity=None,npatches=None):
    """
    Filters out columns from pandas DataFrame generated by run_and_save()
    """

    lab = df.iloc[0]['learning_method']
    if lab in ['haar-dict','centroids-dict']:
        clust = df.iloc[0]['clustering']
        lab += '-'+clust
        if clust == 'spectral':
            lab += '-'+df.iloc[0]['similarity_measure']

    if df.iloc[0]['clustering'] == 'spectral': #comment this to plot also dictionaries using spectral clustering
        return(None,None)
    #if df.iloc[0]['clustering'] != 'spectral' and df.iloc[0]['clustering'] != 'twomeans':
    #if df.iloc[0]['learning_method'] == 'centroids-dict': 
    #    return(None,None)
    #if df.iloc[0]['learning_method'] == 'ksvd': 
    #    return(None,None)
    #if psize is not None:
    #    df = df[df['patch_size'] == psize]
    #df = df[df['clustering'] != 'spectral']
    #if index != 'reconstruction_sparsity':
    if index == 'n.patches':
        if reconstruction_sparsity is None:
            raise Exception('If using n.patches as index, reconstruction_sparsity should be explicitly set')
        df = df[df['reconstruction_sparsity'] == reconstruction_sparsity]
        #ipdb.set_trace()
        df = df[df['patch_size'] == psize]

    elif index == 'reconstruction_sparsity':
        if npatches is None:
            raise Exception('If using reconstruction_sparsity as index, npatches should be explicitly set')
        df = df[df['n.patches'] == npatches]
    elif index == 'patch_size':
        #if npatches is None or reconstruction_sparsity is None:
        #    raise Exception('If using patch_size as index, both npatches and reconstruction_sparsity should be explicitly set')
        #if reconstruction_sparsity is None:
        #    raise Exception('If using patch_size as index, reconstruction_sparsity should be explicitly set')
        newdf = pd.DataFrame(columns = df.columns)
        grouped_dfs = df.groupby('patch_size')
        #toplot = [v for k,v in df.fillna('NaN').groupby(['learning_method','clustering','similarity_measure'])]
        for ps,d in grouped_dfs:
            npatches = d['n.patches'].iloc[-1]
            d = d[d['n.patches'] == npatches]
            sparsities = list(set(d['reconstruction_sparsity']))
            sparsities.sort()
            reconstruction_sparsity = sparsities[5]
            d = d[d['reconstruction_sparsity'] == reconstruction_sparsity]
            newdf.append(d)
            #df = df[df['n.patches'] == npatches & df['reconstruction_sparsity'] == reconstruction_sparsity]
        df = newdf
    return(df,lab)


def setAxLinesBW(ax):
    """
    Take each Line2D in the axes, ax, and convert the line style to be 
    suitable for black and white viewing.

    source: https://stackoverflow.com/questions/7358118/matplotlib-black-white-colormap-with-dashes-dots-etc
    """

    lines_to_adjust = ax.get_lines()
    try:
        lines_to_adjust += ax.get_legend().get_lines()
    except AttributeError:
        pass

    for line in lines_to_adjust:
        lsty = COLORMAP[line.get_label()]            
        line.set_color('black')
        line.set_dashes(lsty['dash'])
        line.set_marker(lsty['marker'])
        line.set_linewidth(lsty['lwidth'])
        line.set_markersize(MARKERSIZE)

def setFigLinesBW(fig):
    """
    Take each axes in the figure, and for each line in the axes, make the
    line viewable in black and white.

    source: https://stackoverflow.com/questions/7358118/matplotlib-black-white-colormap-with-dashes-dots-etc
    """
    for ax in fig.get_axes():
        setAxLinesBW(ax)

def run_and_save(fpath = None):
    """
    Run batch tests. Returns pandas DatFrame with test results and list of instances that was not possible to reconstruct (OMP is finnicky). If fpath is set a pickle of this is saved. 
    """
    
    learnimgs = 'img/flowers_pool-rescale.npy'
    #learnimgs = 'img/boat512.png'
    #learnimgs = ['img/cameraman256.png','img/lena512.png','img/barbara512.png','img/peppers256.png']

    #codeimg = 'img/landscape2-rescaled.jpg'
    codeimg = 'img/flowers_pool-rescale.npy'
    #codeimg = 'img/cameraman256.png'

    #patch_sizes_npatches_dictsize = [((8,8),15000),((12,12),10000),((16,16),8000),((24,24),4000),((32,32),1500)]
    #patch_sizes_npatches = [((8,8),150),((12,12),100)]
    #patch_sizes_npatches = [((32,32),int(2e4))]
    #patch_sizes_npatches = [((8,8),int(2550))]
    #npatches = np.arange(150,2e4,600).astype('int')
    #npatches = np.arange(50,1500,50).astype('int')
    npatches4 = np.logspace(1.8,3.5,30).astype('int')
    npatches8 = np.logspace(2,3.65,30).astype('int')
    npatches16 = np.logspace(2.5,3.8,30).astype('int')
    npatches32 = np.logspace(3.1,4,30).astype('int')
    patch_sizes_npatches = [((4,4),k) for k in npatches4]
    patch_sizes_npatches += [((8,8),k) for k in npatches8]
    patch_sizes_npatches += [((16,16),k) for k in npatches16]
    patch_sizes_npatches += [((32,32),k) for k in npatches32]
    overlap = True
    df_saveable = Saveable()
    df_saveable.df = pd.DataFrame()
    errors = []
    if fpath is not None:
        now = dt.datetime.now()
        timestr = '-'.join(map(str,[now.year,now.month,now.day])) + '_'+':'.join(map(str,[now.hour,now.minute,now.second]))
        df_fpath = '-'.join([fpath,timestr,'df.pickle'])
    for i,psize_np in enumerate(patch_sizes_npatches):
        psize,npat = psize_np
        newtests = 0
        pdim = psize[0]*psize[1]
        dsize = max(1,min(int(pdim*1.5),int(npat/2))) #dictionary 50% bigger than dimension
        spars = max(1,min(int(pdim*0.01),int(npat/2))) #sparsity 1% of dimension
        cur_tests = []
        for meth in ['ksvd', 'haar-dict','centroids-dict']:
        #for meth in ['haar-dict']:
            if meth == 'ksvd':
                ct = Test(learnimgs,npat,psize,noisevar=0,overlapped_patches=overlap)
                ct.debug = True
                ct.learn_dict(method=meth, dictsize=dsize, ksvdsparsity=3*spars)
                cur_tests.append(ct)
            else:
                for clust in ['twomeans','twomaxoids','spectral']:
                #for clust in ['spectral']:
                    if clust == 'spectral' and npat < 800: #spectral clustering is slow
                        for simm in ['frobenius','haarpsi','emd']:
                            ct = Test(learnimgs,npat,psize,noisevar=0,overlapped_patches=overlap)
                            ct.debug = True
                            ct.learn_dict(method=meth, dictsize=dsize, clustering=clust, spectral_similarity=simm)
                            cur_tests.append(ct)
                    elif clust != 'spectral':
                        ct = Test(learnimgs,npat,psize,noisevar=0,overlapped_patches=overlap)
                        ct.debug = True
                        ct.learn_dict(method=meth, dictsize=dsize, clustering=clust)
                        cur_tests.append(ct)
        for k in range(1,15):
            if k*spars > int(dsize/2):
                continue
            for t in cur_tests:
                t.overlapped_patches = False
                success = False
                try:
                    t.reconstruct(codeimg,k*spars)
                    success = True
                except:
                    errors += [(psize,npat,t.learning_method)]
                if success:
                    t._compute_test_results()
                    t.print_results()
                    df_saveable.df = df_saveable.df.append(t.test_results,ignore_index=True)
                    if fpath is not None:
                        df_saveable.save_pickle(df_fpath)                    
                    newtests += 1
            gc.collect()
    return(df_saveable.df,errors)

                
def plot_time(df,index='n.patches',psize=None,reconstruction_sparsity=None,npatches=None):
    """
    Plots reconstruction times from a pandas DataFrame generated by run_and_save().

    df: the DataFrame
    index: the column to use as index. Tested with 'n.patches','patch_size' and 'reconstruction_sparsity'
    """
    
    minx,maxx = None,None
    toplot = [v for k,v in df.fillna('NaN').groupby(['learning_method','clustering','similarity_measure'])]
    #toplot = [v for k,v in df.fillna('NaN').groupby(['learning_method','clustering'])]
    for cur_df in toplot:
        #STEP 1: further filter cur_df's columns
        cur_df,lab = _filter_columns(cur_df,index,psize,reconstruction_sparsity,npatches)
        if cur_df is None:
            continue
        time = cur_df.set_index(index)['learning_time']
        #following is used to later set ylim for plot
        if set(cur_df['learning_method']) == set(['ksvd']):
            maxy = cur_df['learning_time'].max()
        #following is used to later set xlim for plot
        if set(cur_df['clustering']) == set(['spectral']):
            maxx = cur_df['n.patches'].max()
            minx = cur_df['n.patches'].min()
        if index == 'patch_size':
            ticks = [str(k) for k in time.index]
            plt.xticks([0],ticks)
        #time.plot(label=lab,style='x--')
        time.plot(label=lab)
    plt.legend(loc='best')
    #plt.yscale('log')
    #plt.ylim(0,maxtime*1.2)
    if minx is not None and maxx is not None:
        plt.xlim(minx*0.9,maxx*1.2)
    plt.legend(loc='best')
    #plt.legend(loc='upper right')
    plt.grid()
    setFigLinesBW(plt.gcf())
    plt.show()

def plot_rec_quality(df,index='n.patches',psize=None,reconstruction_sparsity=None,npatches=None):
    """
    Plots HaarPSI of reconstructed images from a pandas DataFrame generated by run_and_save().

    df: the DataFrame
    index: the column to use as index. Tested with 'n.patches','patch_size' and 'reconstruction_sparsity. 
    When set to 'patch_size' no other arguments are needed.
    """

    toplot = [v for k,v in df.fillna('NaN').groupby(['learning_method','clustering','similarity_measure'])]
    #toplot = [v for k,v in df.fillna('NaN').groupby(['learning_method','clustering'])]
    for cur_df in toplot:
        #STEP 1: further filter cur_df's columns
        cur_df,lab = _filter_columns(cur_df,index,psize,reconstruction_sparsity,npatches)
        if cur_df is None:
            #print('DataFrame is empty')
            continue
        #print('DataFrame is not empty')
        quality = cur_df.set_index(index)['haarpsi']
        #ipdb.set_trace()
        #if index == 'patch_size':
        #    ticks = [str(k) for k in quality.index]
        #    plt.xticks([0],ticks)
        #quality.plot(logy=True,label=lab,style='x--')
        quality.plot(label=lab)
    plt.legend(loc='best')
    #plt.xscale('log')
    #plt.xlim(0,850)
    plt.grid()
    setFigLinesBW(plt.gcf())
    plt.show()

            
if __name__ == '__main__':
    run_and_save_pickle(sys.argv[1])
